{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient validation and finite differences\n",
    "======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import Orange\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = Orange.data.Table(\"data/age-sbp.tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = data.X, data.Y\n",
    "X = np.column_stack((np.ones(len(x)), x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytical gradient, gradient descent and cost function for linear regression\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad(X, y, theta):\n",
    "    return (X.dot(theta) - y).dot(X)\n",
    "\n",
    "def gradient_descent(X, y, alpha=0.0001, epochs=1000):\n",
    "    \"\"\"For a matrix x and vector y return a linear regression model.\"\"\"\n",
    "    theta = np.zeros(X.shape[1]).T\n",
    "    for i in range(epochs):\n",
    "        theta = theta - alpha * grad(X, y, theta) / len(X)\n",
    "    return theta\n",
    "\n",
    "def J(X, y, theta):\n",
    "    return 0.5 * sum((X.dot(theta) - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 98.04582353   0.98421061]\n"
     ]
    }
   ],
   "source": [
    "theta = gradient_descent(X, y, alpha=0.0005, epochs=100000)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient checking through comparison with finite differences\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test our gradient of the cost function J and our Python implementation are correct. Namely, alternatively, we can compute the gradient with the method of [finite differences](https://en.wikipedia.org/wiki/Finite_difference), and then compare the two solutions. For this to work, we need an function that computes the cost function J (above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_approx(X, y, theta, e=1e-1):\n",
    "    return np.array([(J(X, y, theta+eps) - J(X, y, theta-eps))/(2*e)\n",
    "                     for eps in np.identity(len(theta)) * e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  -61444. -3064664.]\n",
      "[  -61443.99999991 -3064663.99999999]\n"
     ]
    }
   ],
   "source": [
    "theta = np.array([-10, -42])\n",
    "print(grad(X, y, theta))\n",
    "print(grad_approx(X, y, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works fine. Great! We could use this way of computing the gradient in the first place, and then avoid all the \"unnecessary\" mathematics for computing the analytical solution. Right? Wrong! Computing the gradient through finite differences is much slower. Consider, how many times we need to evaluate the cost function. Also, the finite differences approximation depends on parameter epsilon, which in general we do not know how to set appropriately, for a given J and theta. If there exists an analytical solution for the gradient, we should use it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
