{
 "metadata": {
  "name": "",
  "signature": "sha256:436adf48d70d763443eaa09e32369a3b249ffa78e27f4591fbe178f8f11566f5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Finite Differences Revisited "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Derivatives calculated by hand (or in any other way, which does not generate executable code) should be compared with those obtained using **central differences**. This allows us to check on the correctness of algorithm implementation.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Central differences:"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "$$\\frac{\\partial J}{\\partial \\boldsymbol{\\theta}_{i}} = \\frac{J(\\boldsymbol{\\theta}_{i} + \\epsilon) - J(\\boldsymbol{\\theta}_i - \\epsilon)}{2\\epsilon} + O(\\epsilon^2),$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where $J$ is cost function, $\\boldsymbol{\\theta}$ are model parameters and $\\epsilon \\ll 1$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\phantom{0}\\\\[10cm]$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "A Simple Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose we are given a cost function:"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "$$ J(\\boldsymbol{\\theta}) = - \\frac{1}{n}(\\sum_{i=1}^{n} y^{(i)} \\log h_\\boldsymbol{\\theta}(x^{(i)}) + (1-y^{(i)}) \\log (1 - h_\\boldsymbol{\\theta}(x^{(i)}))), $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where $n$ is the number of examples, $y^{(i)}$ and $x^{(i)}$ are variables, and $$h_\\boldsymbol{\\theta}(x) = \\frac{1}{1 + \\exp(- \\boldsymbol{\\theta)}^T x}.$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Say, we take the derivatives of $J(\\boldsymbol{\\theta})$ and find the gradient $\\nabla_{\\boldsymbol{\\theta}_i} J(\\boldsymbol{\\theta})$. We implement it in a function ``grad``."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*(Sidenote: this is softmax regression model without regularization)*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "import Orange"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Our cost function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cost(X, Y, theta):\n",
      "    Theta = theta.reshape(Y.shape[1], X.shape[1])\n",
      "    M = np.dot(X, Theta.T)\n",
      "    P = np.exp(M - np.max(M, axis=1)[:, None])\n",
      "    P /= np.sum(P, axis=1)[:, None]\n",
      "    cost = -np.sum(np.log(P) * Y) / X.shape[0]\n",
      "    return cost"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Our gradient function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ?\n",
      "def grad(X, Y, theta):\n",
      "    Theta = theta.reshape(Y.shape[1], X.shape[1])\n",
      "    M = np.dot(X, Theta.T)\n",
      "    P = np.exp(M - np.max(M, axis=1)[:, None])\n",
      "    P /= np.sum(P, axis=1)[:, None]\n",
      "    grad = np.dot(X.T, P).T / X.shape[0]\n",
      "    return grad.ravel()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Central differences "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def numerical_grad(f, params, epsilon):\n",
      "    num_grad = np.zeros_like(theta)\n",
      "    perturb = np.zeros_like(params)\n",
      "    for i in range(params.size):\n",
      "        perturb[i] = epsilon\n",
      "        j1 = f(params + perturb)\n",
      "        j2 = f(params - perturb)\n",
      "        num_grad[i] = (j1 - j2) / (2. * epsilon)\n",
      "        perturb[i] = 0\n",
      "    return num_grad"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Let's get some data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = Orange.data.Table('iris')\n",
      "X = data.X\n",
      "Y = np.eye(3)[data.Y.astype(int)]  \n",
      "theta = np.random.randn(3 * 4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Compare derivatives returned by ``grad`` function with those obtained by central differences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compare analytical with numerical \n",
      "ag = grad(X, Y, theta)\n",
      "ng = numerical_grad(lambda params: cost(X, Y, params), theta, 1e-4)\n",
      "print(np.sum((ag - ng)**2))\n",
      "ag"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20.9929182282\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "array([  7.26178762e-04,   4.83437845e-04,   2.23341555e-04,\n",
        "         3.63379361e-05,   1.84465393e+00,   1.21731377e+00,\n",
        "         6.21499878e-01,   1.25926418e-01,   3.99795322e+00,\n",
        "         1.83620279e+00,   3.13694345e+00,   1.07270391e+00])"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ng"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "array([-1.65843276, -1.13701624, -0.48763135, -0.08129259, -1.97743703,\n",
        "       -0.92308212, -1.41996049, -0.44199544,  3.6359416 ,  2.06010238,\n",
        "        1.90759188,  0.52328803])"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}