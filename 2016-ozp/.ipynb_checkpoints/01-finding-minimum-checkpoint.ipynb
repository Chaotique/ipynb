{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from scipy.optimize import fmin_l_bfgs_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum of a function with single parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single-attribute function J we would like to find the value of its attribute where the function has its minima. First, we define the function and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def J(theta):\n",
    "    # return theta**4 + 4*theta**3 - 2\n",
    "    return theta**2 - 4*theta -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b81c208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "thetas = np.arange(-10, 10, 0.1)\n",
    "plt.plot(thetas, J(thetas));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a method of finite diference to approximate a derivative of the function. Then, we check this function if it gives sensible results for J, our target function. We then analytically compute the derivative and check if the two means of finding the derivative give approximately the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fdm(f, x, eps=1e-5):\n",
    "    \"\"\"Approximates derivate of f in x by finite differences method\"\"\"\n",
    "    return (f(x+eps) - f(x-eps))/(2*eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4., -16., -20.])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas = np.array([4, -6, -8])\n",
    "fdm(J, thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dJ(theta):\n",
    "    # return 4*theta**3 + 12*theta**2\n",
    "    return 2*theta - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4, -16, -20])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ(thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the minimum, we start at some initial value of the parameter x0. Then we compute the derivative at this point, and move in the direction opposite to derivative. We should take care to move in a small step, not to overshoot our target, the minimum of a function. Note that close to the minimum the function flattens, the derivative is close to zero, so the corrections of the parameter value are small and the procedure converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_min(df, x0, learning_rate=1e-3, err=1e-3):\n",
    "    \"\"\"Returns a minimum of a function with derivative df\"\"\"\n",
    "    x = x0\n",
    "    while True:\n",
    "        x_new = x - learning_rate * df(x)\n",
    "        if np.linalg.norm(x-x_new) < err:\n",
    "            break\n",
    "        x = x_new\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9995046101466407"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_min(lambda x, f=J: fdm(f, x), 0, learning_rate=1e-2, err=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.999508406287749"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_min(dJ, -6, learning_rate=1e-2, err=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got very similar results with approximation of derivative and the functiona that uses analytical value of derivative. The variant with the analytically computed derivative is faster, though. Why? Finally, we also use an [optimization function from scipy](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.fmin_l_bfgs_b.html) that converges much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.]),\n",
       " array([-9.]),\n",
       " {'funcalls': 3,\n",
       "  'grad': array([ 0.]),\n",
       "  'nit': 2,\n",
       "  'task': b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmin_l_bfgs_b(J, 0, dJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum of a function with more (actually, two) attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extend the procedures to functions with more than one attribute. We here do it for the two-attribute function, but the code is amiable for functions with any number of attributes. The attributes theta is now a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def J(theta):\n",
    "    return (theta[0] - 1)**2 + (2*theta[1] + 3)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dJ(theta):\n",
    "    return np.array([2*(theta[0]-1), 4*(2*theta[1]+3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J(np.array([-1,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9995077, -1.5      ])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta0 = np.array([0, 0])\n",
    "# find_min(lambda x, f=J: fdm(f, x), np.array([0, 0]), learning_rate=1e-2, err=1e-5)\n",
    "find_min(dJ, theta0, learning_rate=1e-2, err=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.00000344, -1.50000053]),\n",
       " 1.2982344753279782e-11,\n",
       " {'funcalls': 7,\n",
       "  'grad': array([  6.88905043e-06,  -4.22864669e-06]),\n",
       "  'nit': 6,\n",
       "  'task': b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmin_l_bfgs_b(J, theta0, dJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the gradient computed by the [finite difference](https://en.wikipedia.org/wiki/Finite_difference) method with a analyticially computed gradient. Functions for which we seek the minima are sometimes complex, and it is good to know that our analytical solution is right, that is, that it gives the correct result. Its also ok to check if the results are ok if the values of theta vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_approx(f, theta, eps=1e-1):\n",
    "    return np.array([(f(theta+e) - f(theta-e))/(2*eps)\n",
    "                     for e in np.identity(len(theta)) * eps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.,  16.])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas = np.array([0.5, 0.5])\n",
    "grad_approx(J, thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.,  16.])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
